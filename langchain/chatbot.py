import os
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, StateGraph
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict
from typing import Sequence
from langchain_core.messages import trim_messages, AIMessage

api = os.environ.get("OPEN_AI_TOKEN")
model = init_chat_model("gpt-5-mini", model_provider="openai"
                         , base_url="https://aihubmix.com/v1", api_key=api)
class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    language: str


prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer all questions to the best of your ability in {language}.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)





#  =============== Actually Work

trimmer = trim_messages(
    max_tokens=65,
    strategy="last",
    token_counter=model,
    include_system=True,
    allow_partial=False,
    start_on="human",
)

def call_model(state: State):
    trimmed_messages = trimmer.invoke(state["messages"])
    prompt = prompt_template.invoke(
        {"messages": trimmed_messages, "language": state["language"]}
    )
    response = model.invoke(prompt)
    return {"messages": response}




# Define the (single) node in the graph
workflow = StateGraph(state_schema=State)
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)
app = workflow.compile(checkpointer=MemorySaver())


config = {"configurable": {"thread_id": "abc123"}}

def chat_loop():
    """å¯åŠ¨å‘½ä»¤è¡ŒèŠå¤©å¾ªç¯"""
    print("ğŸ¤– èŠå¤©æœºå™¨äººå·²å¯åŠ¨ï¼")
    print("æ”¯æŒçš„å‘½ä»¤ï¼š")
    print("  /language [lang] - åˆ‡æ¢è¯­è¨€ (ä¾‹å¦‚: /language Chinese)")
    print("  /quit - é€€å‡ºèŠå¤©")
    print("  /help - æ˜¾ç¤ºå¸®åŠ©")
    print()

    language = "Chinese"  # é»˜è®¤ä¸­æ–‡
    print(f"å½“å‰è¯­è¨€: {language}")
    print("å¼€å§‹èŠå¤©å§ï¼è¾“å…¥æ‚¨çš„é—®é¢˜...")
    print("-" * 50)

    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚¨: ").strip()

            if not user_input:
                continue

            # å¤„ç†å‘½ä»¤
            if user_input.startswith("/"):
                if user_input == "/quit" or user_input == "/exit":
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                elif user_input.startswith("/language"):
                    parts = user_input.split(maxsplit=1)
                    if len(parts) > 1:
                        language = parts[1]
                        print(f"ğŸŒ å·²åˆ‡æ¢åˆ° {language}")
                    else:
                        print("âŒ ç”¨æ³•: /language [è¯­è¨€]")
                    continue
                elif user_input == "/help":
                    print("ğŸ“‹ å¯ç”¨å‘½ä»¤:")
                    print("  /language [lang] - åˆ‡æ¢è¯­è¨€")
                    print("  /quit, /exit - é€€å‡º")
                    print("  /help - æ˜¾ç¤ºæ­¤å¸®åŠ©")
                    continue
                else:
                    print("â“ æœªçŸ¥å‘½ä»¤ï¼Œè¾“å…¥ /help æŸ¥çœ‹å¸®åŠ©")
                    continue

            # å¤„ç†æ­£å¸¸å¯¹è¯
            input_messages = [HumanMessage(user_input)]
            
            # ä½¿ç”¨æµå¼è¾“å‡º
            print("ğŸ¤– AI: ", end="", flush=True)
            full_response = ""
            
            try:
                # å…ˆè°ƒç”¨å·¥ä½œæµæ›´æ–°çŠ¶æ€ï¼Œä½†ä½¿ç”¨ç©ºå“åº”ï¼ˆå®é™…å“åº”æ¥è‡ªæµå¼è¾“å‡ºï¼‰
                output = app.invoke({"messages": input_messages, "language": language}, config)
                
                # è·å–æœ€æ–°çš„æ¶ˆæ¯å†å²ï¼ˆåŒ…å«ç”¨æˆ·è¾“å…¥ï¼‰
                trimmed_messages = trimmer.invoke(output["messages"])
                prompt = prompt_template.invoke(
                    {"messages": trimmed_messages, "language": language}
                )
                
                # ä½¿ç”¨æ¨¡å‹çš„æµå¼è¾“å‡º
                for chunk in model.stream(prompt):
                    if chunk.content:
                        print(chunk.content, end="", flush=True)
                        full_response += chunk.content

                print()  # æ¢è¡Œ
                
                # å°†AIå›å¤æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­
                if full_response:
                    app.invoke({"messages": [AIMessage(content=full_response)], "language": language}, config)
                
            except Exception as e:
                # å¦‚æœæµå¼è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šè¾“å‡º
                print(f"\nâš ï¸  æµå¼è¾“å‡ºå¤±è´¥ï¼Œä½¿ç”¨æ™®é€šè¾“å‡º: {e}")
                output = app.invoke({"messages": input_messages, "language": language}, config)
                ai_response = output["messages"][-1].content
                print(f"ğŸ¤– AI: {ai_response}")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

# å¦‚æœæ˜¯ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œå¯åŠ¨èŠå¤©å¾ªç¯
if __name__ == "__main__":
    chat_loop()